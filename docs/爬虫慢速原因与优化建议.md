# 爬虫慢速原因分析与优化建议

## 一、日志与耗时概况

- **任务开始**: 2026-01-30 20:31:38  
- **STEP 2 结束**: 2026-01-30 23:14:17  
- **总耗时**: 约 **2 小时 43 分钟**  
- **处理数量**: **约 4000 只股票**（第 1 个 → 第 4000 个）  
- **平均每只**: 约 **2.44 秒/只**

## 二、根本原因

### 1. 完全串行、无并发

当前使用 **baostock** 数据源时，在 `stock_kline.py` 的 `start_requests()` 里：

- 对 **每一只股票** 在同一个 for 循环里依次执行：
  1. `fetch_kline_data_baostock_simple()` — 拉 K 线（**阻塞网络 I/O**）
  2. `get_stock_name_baostock()` — 拉股票名称（**又一次阻塞网络 I/O**）
  3. `process_kline_data()` — 算指标、分析信号、写库（CPU + 本地 I/O）

因此是 **严格的一只接一只**，没有任何并行。

### 2. Scrapy 的并发配置未生效

- `settings.py` 里配置了 `CONCURRENT_REQUESTS = 16`、`DOWNLOAD_DELAY = 1`。
- 这些只对 **Scrapy 的 Request/Response** 生效。
- 使用 baostock 时，**没有 yield scrapy.Request**，所有工作都在 `start_requests()` 的 for 循环里用同步函数做完，所以 **Scrapy 的并发和下载延迟都不会被用到**，等价于单线程串行。

### 3. 每只股票两次网络请求

- 每只股票至少：一次 K 线接口 + 一次股票基本信息（名称）接口。
- 每次请求受网络和 baostock 服务响应时间影响，通常 0.5～2+ 秒，叠加后整体就慢。

### 4. 数据量本身很大

- 约 4000 只 × 2.44 秒/只 ≈ 9760 秒 ≈ 2.7 小时，与日志中的 2h43min 一致，说明主要时间都花在「每只串行 + 两次请求 + 本地计算」上。

## 三、优化方向建议

| 方向 | 说明 | 预期效果 |
|------|------|----------|
| **1. 多进程/多线程并行** | 用 `concurrent.futures.ThreadPoolExecutor` 或 `ProcessPoolExecutor` 对多只股票同时拉 K 线、取名称、再处理（注意 baostock 是否线程安全，必要时用进程池）。 | 在 8～16 并发下，总耗时有望降到约 **15～30 分钟** 量级。 |
| **2. 股票名称批量/缓存** | 若 baostock 支持按列表查基础信息，可先批量拉一次股票名称并缓存；或首次查后写本地缓存，避免每只都调 `get_stock_name_baostock`。 | 每只少 1 次网络请求，整体再减少约 **10%～30%** 时间。 |
| **3. 仅必要时写库** | 在 `process_kline_data` 里对「无信号」或「不满足输出条件」的股票尽量减少或跳过 DB 的更新/写入。 | 降低 I/O，对 4000 只规模会有一定加速。 |
| **4. 可选：换回东方财富 + Scrapy** | 若改用东方财富等 HTTP API，并用 Scrapy Request，则 `CONCURRENT_REQUESTS`、`DOWNLOAD_DELAY` 会生效，可 16 并发爬取（需注意对方限频）。 | 能利用现有 Scrapy 并发配置，但需改数据源与解析逻辑。 |

建议优先做 **1（并行）** 和 **2（名称缓存/批量）**，改动集中、收益大。

## 四、小结

- **慢的直接原因**：baostock 路径下 **全串行** + **每只两次网络请求** + **约 4000 只**，且 Scrapy 并发未参与。
- **最有效手段**：在保持 baostock 的前提下，对「多只股票」做 **并行请求与处理**（多线程/多进程），并尽量 **减少或合并股票名称的请求**（批量/缓存）。
